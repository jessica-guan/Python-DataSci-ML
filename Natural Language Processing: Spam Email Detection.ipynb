{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7dzC09dLlEhm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-guan/Python-DataSci-ML/blob/main/Natural%20Language%20Processing%3A%20Spam%20Email%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 20: Natural Language Processing I**\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "In this week's homework, you will apply what you learned in this week's lab to classify emails as either `\"spam\"` or `\"not spam\"`.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Structure**\n",
        "**Part 1**: Detecting Spam Emails with a DNN\n",
        "\n",
        "**Part 2**: Detecting Spam Emails with a CNN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Cheat Sheets**\n",
        "[Natural Language Processing I](https://docs.google.com/document/d/1ZaLtMF7aQsG05myetJpoTJlr-sAIURP_a9sQr66pfqw/edit?usp=drive_link)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**"
      ],
      "metadata": {
        "id": "N61ZizxuBc3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "YAvvLhRIoqYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Part 1: Detecting Spam Emails with a DNN**\n",
        "---\n",
        "\n",
        "\n",
        "**Run the code provided below to import the dataset.**"
      ],
      "metadata": {
        "id": "idga37M2FsMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('https://docs.google.com/spreadsheets/d/e/2PACX-1vQaAZH50du-EP6meYf_LjHztynjYFZ2mg1miSvjgz8nLNh_lnbSdgARSQC10UdhhQ/pub?output=xlsx')\n",
        "\n",
        "inputs = df[[\"Column2\"]]\n",
        "output = df[\"Column1\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, output, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "In9BSfqd_OPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.1: Create the `TextVectorization` layer**\n",
        "\n",
        "\n",
        "Let's create a `TextVectorization` layer to vectorize this data.\n",
        "\n",
        "Specifically,\n",
        "1. Initialize the layer with the specified parameters.\n",
        "\n",
        "2. Adapt the layer to the training data."
      ],
      "metadata": {
        "id": "E90o2LJcwsMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Initialize the layer with the specified parameters.**\n",
        "\n",
        "* `max_tokens = 5000`\n",
        "* `output_mode = 'int'`\n",
        "* `output_sequence_length = 50`"
      ],
      "metadata": {
        "id": "koGBxiapJC_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = 5000,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 50\n",
        "  )\n",
        "\n",
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "SVG1Sy7OJU0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.2: Look at the vocabulary**\n",
        "\n",
        "\n",
        "Print the first 50 words of the vocabulary."
      ],
      "metadata": {
        "id": "Vz2dKD8pJhXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "3pDiV7anAv60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed13f938-98c7-427a-e3ec-6e385ab76417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'for',\n",
              " 'you',\n",
              " 'this',\n",
              " 'that',\n",
              " 'enron',\n",
              " 'will',\n",
              " 'with',\n",
              " 'ect',\n",
              " 'have',\n",
              " 'from',\n",
              " 'are',\n",
              " 'your',\n",
              " 'hou',\n",
              " 'our',\n",
              " 'not',\n",
              " 'would',\n",
              " 'has',\n",
              " 'all',\n",
              " 'ees',\n",
              " 'please',\n",
              " 'can',\n",
              " 'any',\n",
              " 'com',\n",
              " 'they',\n",
              " 'but',\n",
              " 'out',\n",
              " 'get',\n",
              " 'was',\n",
              " 'more',\n",
              " 'power',\n",
              " 'been',\n",
              " 'which',\n",
              " 'also',\n",
              " 'energy',\n",
              " 'some',\n",
              " 'kitchen',\n",
              " 'one',\n",
              " 'these',\n",
              " 'time',\n",
              " 'there',\n",
              " 'over',\n",
              " 'need',\n",
              " 'subject',\n",
              " 'what',\n",
              " 'know',\n",
              " 'their']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.3: Add the input and text vectorization layers to the model**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTCj-Z7rJ5on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)"
      ],
      "metadata": {
        "id": "RYh5A78OKRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.4: Look at the vectorization of an example**\n",
        "\n",
        "\n",
        "Add your own sentence below to see how it would be vectorized with our newly adapted layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** `TextVectorizer` will ignore any punctuation and consider upper and lower case the same. There are extra parameters that can set to adjust this."
      ],
      "metadata": {
        "id": "Ohs_ludzKcE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_0 = model.predict([\"Go Bruins!\"])\n",
        "\n",
        "print(vector_0)\n",
        "print(vector_0.shape)"
      ],
      "metadata": {
        "id": "PZoit5S6OnPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d4043c-4f35-40ff-976b-ee116aedfe32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "(1, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***STOP!* Answer the following question under Problem #8: When using the text vectorization layer in Keras, what do the numbers in the vectorization represent?**"
      ],
      "metadata": {
        "id": "fSpGeUg0RoBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.5: Add hidden layers and an output layer**\n",
        "\n",
        "\n",
        "Add two dense layers with 512 neurons and ReLU activation.\n",
        "\n",
        "Then, because this is a binary classification task, create the output layer with a single neuron (for spam/not spam) and the sigmoid activation function."
      ],
      "metadata": {
        "id": "eN8Z-C_3K0ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "liLaJe3dLPN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our DNN."
      ],
      "metadata": {
        "id": "zYru7HAFGz3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI8Yhy61G2Ar",
        "outputId": "ea32f5a4-84f4-48b9-fbab-50732e960eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, 50)                0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               26112     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 289281 (1.10 MB)\n",
            "Trainable params: 289281 (1.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.6: Compile and fit the model**\n",
        "\n",
        "\n",
        "Compile and fit the model with the following parameters:\n",
        "* Adam learning rate of 0.001\n",
        "* `binary_crossentropy` for the loss function\n",
        "* Accuracy as the metric\n",
        "* For the fit, use `epochs=5` and `batch_size=128`"
      ],
      "metadata": {
        "id": "fuGPjq1bLSNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "id": "rjiBv6JzLk4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddc52f0-0395-4ed1-b5ad-df5453dfb554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 [==============================] - 3s 12ms/step - loss: 333.3740 - accuracy: 0.5100\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 128.0046 - accuracy: 0.5537\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 61.0992 - accuracy: 0.5962\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 42.0817 - accuracy: 0.6625\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 23.3307 - accuracy: 0.7025\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b0788184fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***STOP!* Answer the following question under Problem #9: Why are we using the binary crossentropy loss function instead of the categorical crossentropy function for this task?**"
      ],
      "metadata": {
        "id": "CFEwGmJRTOyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #1.7: Evaluate the model**\n",
        "\n",
        "\n",
        "Now, evaluate the model for both the training and test sets.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** As a baseline, randomly guessing 1 out of 4 possible classes would achieve a roughly 0.25 accuracy."
      ],
      "metadata": {
        "id": "JSOOynZqL0xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the training set\n",
        "model.evaluate(x_train, y_train)\n",
        "\n",
        "# Evaluate the test set\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "HqPSJlsSL7_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93420ce2-ad29-4962-8de7-b717d4dc1d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 7ms/step - loss: 22.1104 - accuracy: 0.7075\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 51.3896 - accuracy: 0.5150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51.38957214355469, 0.5149999856948853]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Part 2: Detecting Spam Emails with a CNN**\n",
        "---\n"
      ],
      "metadata": {
        "id": "r4_U4kWylKLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #2.1: Initialize the model with an input and vectorizer layer**\n",
        "\n",
        "\n",
        "*Hint: This is the same as last time.*"
      ],
      "metadata": {
        "id": "VN8Z0gy_sExR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "\n",
        "cnn_model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "cnn_model.add(vectorize_layer)"
      ],
      "metadata": {
        "id": "LvS3JpkcsZI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #2.2: Finish building the CNN**\n",
        "\n",
        "\n",
        "Build your CNN with the following layers:\n",
        "* a convolutional layer with 128 filters, a kernel size of 5, and ReLU activation\n",
        "* a max pooling layer with a pool size of 2\n",
        "* a convolutional layer with 256 filters, a kernel size of 5, and ReLU activation\n",
        "* a max pooling layer with a pool size of 2\n",
        "* a flatten layer\n",
        "* a dense layer with 512 neurons\n",
        "* the output layer"
      ],
      "metadata": {
        "id": "9kV4xruit_UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The convolution layer requires us to cast the inputs to a different data type\n",
        "# and reshape the input as well. We have done this for you.\n",
        "cnn_model.add(Lambda(lambda x: tf.cast(x, 'float32')))\n",
        "cnn_model.add(Reshape((50, 1)))\n",
        "\n",
        "# Start building your CNN below.\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(512, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))  # Assuming it's a binary classification problem, use 'sigmoid'"
      ],
      "metadata": {
        "id": "8SQd40rbnOrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***STOP!* Answer the following question under Problem #10: Why did we use the Conv1D layer instead of the Conv2D layer?**"
      ],
      "metadata": {
        "id": "Friy0yo6TLiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the completed model."
      ],
      "metadata": {
        "id": "VfFAaOAOizPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqK1H52pi1iZ",
        "outputId": "6dfd6fba-6c57-4842-c34c-33639ce7a33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, 50)                0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " lambda_9 (Lambda)           (None, 50)                0         \n",
            "                                                                 \n",
            " reshape_9 (Reshape)         (None, 50, 1)             0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 46, 128)           768       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 23, 128)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 19, 256)           164096    \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 9, 256)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 512)               1180160   \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1345537 (5.13 MB)\n",
            "Trainable params: 1345537 (5.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #2.3: Compile and fit the model**\n",
        "\n",
        "\n",
        "Compile and fit the model with the same parameters as Part 1."
      ],
      "metadata": {
        "id": "kRhmTL_2lOM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(x_train, y_train, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "id": "RKS-Rxcsla4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b77e54-645c-408f-a9b0-1aba80a9e03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 [==============================] - 4s 47ms/step - loss: 274.5205 - accuracy: 0.5063\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 7.2991 - accuracy: 0.5288\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.3035 - accuracy: 0.5437\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.0806 - accuracy: 0.5975\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.7835 - accuracy: 0.6350\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b0708404e80>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise #2.4: Evaluate the model**\n",
        "\n",
        "\n",
        "Now, evaluate the model for both the training and test sets."
      ],
      "metadata": {
        "id": "FTIEF_GHlGZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.evaluate(x_train, y_train)\n",
        "cnn_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "1VJ42ZnfloOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7677d847-9152-4306-f4cb-c556309b8a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6513\n",
            "7/7 [==============================] - 0s 21ms/step - loss: 0.9629 - accuracy: 0.5650\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9629138112068176, 0.5649999976158142]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#End of notebook\n",
        "\n",
        "Â© 2024 The Coding School, All rights reserved"
      ],
      "metadata": {
        "id": "7dzC09dLlEhm"
      }
    }
  ]
}