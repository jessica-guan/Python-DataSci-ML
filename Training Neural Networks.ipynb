{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-guan/Python-DataSci-ML/blob/main/Training%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 17: Training Neural Networks**\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "In today's lab you'll learn how to train a neural network and you'll experiment with different parameters for fine-tuning deep learning models.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Structure**\n",
        "**Part 1**: [Review](#p1)\n",
        "\n",
        "**Part 2**: [The Wine Dataset](#p2)\n",
        "\n",
        "**Part 3**: [Hyperparameter Tuning](#p3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Learning Objectives**\n",
        "By the end of this lab, you will:\n",
        "* Recognize how to train and evaluate a Neural Network.\n",
        "* Recognize how to do hyperparameter tuning of learning rate, activation functions, and model architecture (layers and neurons).\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **Resources**:\n",
        "* [Deep Learning with keras](https://docs.google.com/document/d/1WCV2ok7dwPWCid5vdOImknCAJS2te5aQ8yRp6J5Clac/edit?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**"
      ],
      "metadata": {
        "id": "mzyUrRqcU0fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "!pip install --quiet keras_visualizer\n",
        "from keras_visualizer import visualizer\n",
        "from IPython.display import Image\n",
        "\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "tRNgYwf5Dobg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"p1\"></a>\n",
        "\n",
        "---\n",
        "## **Part 1: Review**\n",
        "---\n"
      ],
      "metadata": {
        "id": "KElX31obQLfZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r02HhDt5CgUy"
      },
      "source": [
        "#### **Problem #1.1**\n",
        "\n",
        "Create and visualize a new model with three hidden layers.\n",
        "\n",
        "* The input layer should have `10` neurons\n",
        "\n",
        "* The first hidden layer should have `6` neurons with no activation.\n",
        "\n",
        "* The second hidden layer should have `4` neurons with ReLU activation function.\n",
        "\n",
        "* The third hidden layer should have `12` neurons with ReLU activation.\n",
        "\n",
        "* The output layer should have `4` neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeIB6lb2CgUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592423b4-7f6b-4881-ee02-0471c21a89e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 6)                 66        \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4)                 28        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 12)                60        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206 (824.00 Byte)\n",
            "Trainable params: 206 (824.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(10,)),\n",
        "    tf.keras.layers.Dense(6, activation=None),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(12, activation='relu'),\n",
        "    tf.keras.layers.Dense(4)\n",
        "])\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvr43yG7QfNN"
      },
      "source": [
        "#### **Problem #1.2**\n",
        "\n",
        "Create and visualize a new model with three hidden layers.\n",
        "\n",
        "* The input layer should have `8` neurons\n",
        "\n",
        "* The first hidden layer should have `5` neurons with Sigmoid activation.\n",
        "\n",
        "* The second hidden layer should have `8` neurons with ReLU activation function.\n",
        "\n",
        "* The third hidden layer should have `7` neurons with no activation.\n",
        "\n",
        "* The output layer should have `2` neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyzJsUDgQfNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47f3e81-592a-42d0-ca26-74b9b073590c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 5)                 45        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 48        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 7)                 63        \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 16        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 172 (688.00 Byte)\n",
            "Trainable params: 172 (688.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(8,)),\n",
        "    tf.keras.layers.Dense(5, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(7, activation=None),\n",
        "    tf.keras.layers.Dense(2)\n",
        "])\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<center>\n",
        "\n",
        "#### **Wait for Your Instructor to Continue**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XCjfwEA1QvnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a name=\"p2\"></a>\n",
        "\n",
        "---\n",
        "## **Part 2: The Breast Cancer Dataset**\n",
        "---\n",
        "\n",
        "The Breast Cancer Wisconsin (Diagnostic) dataset is a widely used dataset in machine learning for binary classification tasks. It contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The dataset includes 569 instances, each with 30 numeric, real-valued features. These features describe characteristics of the cell nuclei present in the images, such as texture, radius, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
        "\n",
        "The target variable in this dataset is binary, indicating whether the breast mass is malignant or benign. This dataset is particularly significant in medical diagnostic research and machine learning due to its real-world implications in breast cancer diagnosis. It's an excellent resource for teaching machine learning techniques, especially in the context of binary classification problems.\n",
        "\n",
        "**You will use deep learning for classification to predict this variable**:\n",
        "12. `target` (binary value 0 or 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "B9HniB0uu8Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #1: Import and and split the data into training/testing**\n",
        "\n",
        "\n",
        "**This is completed for you. Just run the code below!**"
      ],
      "metadata": {
        "id": "YFYnYOOQvJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "X = data['data']\n",
        "y = data['target']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "WPr5rCaru_Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #2: Determine the dimension of the data**\n",
        "\n"
      ],
      "metadata": {
        "id": "N9PCCGMhWDGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "dimension = X.shape\n",
        "dimension"
      ],
      "metadata": {
        "id": "hRFIIRznvSCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58db767c-70e7-4d69-caf8-c95e7cca19f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3-6: Building the model**\n",
        "\n",
        "\n",
        "Build a NN such that it has:\n",
        "* The correct number of input neurons (one for each feature).\n",
        "* No hidden layers.\n",
        "* Two output neurons with the `'linear'` activation function.\n",
        "\n",
        "Don't forget to visualize the NN."
      ],
      "metadata": {
        "id": "Gm54RjNhWXrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "num_input_features = X.shape[1]\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(num_input_features,)),\n",
        "    tf.keras.layers.Dense(2, activation='linear')\n",
        "])\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8WX_aGLaX-JY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e048355-5a47-4da0-e725-d1aed807522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 2)                 62        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62 (248.00 Byte)\n",
            "Trainable params: 62 (248.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #7: Fit the model**\n",
        "\n",
        "Use the following parameters:\n",
        "\n",
        "* `mse` loss function\n",
        "* accuracy metric\n",
        "* 10 epochs\n",
        "* 0.1 learning rate\n",
        "\n",
        "*Most of the code is given to you.*"
      ],
      "metadata": {
        "id": "5amqSJDgYIFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "r5xUcvXUYbom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14681dd5-e8f2-488e-818e-9f590504b9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - 1s 20ms/step - loss: 9474.2529 - accuracy: 0.5912 - val_loss: 5415.7056 - val_accuracy: 0.7719\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5073.2378 - accuracy: 0.5560 - val_loss: 2783.1309 - val_accuracy: 0.5877\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2889.6328 - accuracy: 0.2681 - val_loss: 1531.1888 - val_accuracy: 0.2632\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1916.0238 - accuracy: 0.1582 - val_loss: 1198.0659 - val_accuracy: 0.1228\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1629.1613 - accuracy: 0.1824 - val_loss: 1123.4020 - val_accuracy: 0.1491\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1538.0336 - accuracy: 0.2154 - val_loss: 1101.4825 - val_accuracy: 0.1491\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1511.0297 - accuracy: 0.2264 - val_loss: 1099.1656 - val_accuracy: 0.1667\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1487.5072 - accuracy: 0.2352 - val_loss: 1070.6849 - val_accuracy: 0.1667\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1462.5448 - accuracy: 0.2264 - val_loss: 1043.6812 - val_accuracy: 0.1667\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1438.7295 - accuracy: 0.2264 - val_loss: 1019.8871 - val_accuracy: 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #8: Evaluate the model**\n",
        "\n"
      ],
      "metadata": {
        "id": "qrLpLzXCIWIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('MSE: ' + str(model.evaluate(X, y)[0]))"
      ],
      "metadata": {
        "id": "VZv_z6q_xB5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486d1614-65cc-496b-c7ea-35bd52b23249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 3ms/step - loss: 1342.2227 - accuracy: 0.2144\n",
            "MSE: 1342.22265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KaM7g0s8m_j"
      },
      "source": [
        "<a name=\"p3\"></a>\n",
        "\n",
        "---\n",
        "## **Part 3: Hyperparameter Tuning**\n",
        "---\n",
        "\n",
        "In this section, we will see how tuning hyperparameters can affect the performance of a Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2waalaIpxBz"
      },
      "source": [
        "### **Problem #3.1**\n",
        "\n",
        "Modify the code below to find the ideal `learning_rate` parameter given everything else is fixed. Consider values such as: `10`, `1`, `0.1`, `0.01`, `0.001`, and `0.0001`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(30),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training model with learning rate: {lr}\")\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "B-GA895WyCr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d4891b-1e72-4417-9560-78cdc25439e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with learning rate: 10\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 2s 74ms/step - loss: 19959853056.0000 - mse: 19959853056.0000 - val_loss: 505.9879 - val_mse: 505.9879\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 860.5072 - mse: 860.5072 - val_loss: 1366.0803 - val_mse: 1366.0803\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1592.9575 - mse: 1592.9575 - val_loss: 1890.4374 - val_mse: 1890.4374\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2011.0984 - mse: 2011.0984 - val_loss: 2164.7100 - val_mse: 2164.7100\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 2223.6233 - mse: 2223.6233 - val_loss: 2297.5093 - val_mse: 2297.5093\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2324.1160 - mse: 2324.1160 - val_loss: 2357.1660 - val_mse: 2357.1660\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 2367.6775 - mse: 2367.6775 - val_loss: 2380.6042 - val_mse: 2380.6042\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 2383.3040 - mse: 2383.3040 - val_loss: 2386.4680 - val_mse: 2386.4680\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 2385.3867 - mse: 2385.3867 - val_loss: 2383.8191 - val_mse: 2383.8191\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 2380.8828 - mse: 2380.8828 - val_loss: 2376.9668 - val_mse: 2376.9668\n",
            "Training model with learning rate: 1\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 28ms/step - loss: 2091.9468 - mse: 2091.9468 - val_loss: 1664.2188 - val_mse: 1664.2188\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1433.1273 - mse: 1433.1273 - val_loss: 1093.8245 - val_mse: 1093.8245\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 918.2417 - mse: 918.2417 - val_loss: 667.3826 - val_mse: 667.3826\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 543.8697 - mse: 543.8697 - val_loss: 372.9545 - val_mse: 372.9545\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 293.5571 - mse: 293.5571 - val_loss: 187.3822 - val_mse: 187.3822\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 141.4822 - mse: 141.4822 - val_loss: 82.4590 - val_mse: 82.4590\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 59.0575 - mse: 59.0575 - val_loss: 30.5826 - val_mse: 30.5826\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 20.4303 - mse: 20.4303 - val_loss: 8.9089 - val_mse: 8.9089\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.3881 - mse: 5.3881 - val_loss: 1.7845 - val_mse: 1.7845\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.9639 - mse: 0.9639 - val_loss: 0.2920 - val_mse: 0.2920\n",
            "Training model with learning rate: 0.1\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 26ms/step - loss: 0.2533 - mse: 0.2533 - val_loss: 0.2378 - val_mse: 0.2378\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2349 - mse: 0.2349 - val_loss: 0.2422 - val_mse: 0.2422\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2358 - mse: 0.2358 - val_loss: 0.2351 - val_mse: 0.2351\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2339 - mse: 0.2339 - val_loss: 0.2364 - val_mse: 0.2364\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2398 - mse: 0.2398 - val_loss: 0.2396 - val_mse: 0.2396\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2348 - mse: 0.2348 - val_loss: 0.2392 - val_mse: 0.2392\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2381 - mse: 0.2381 - val_loss: 0.2354 - val_mse: 0.2354\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2339 - mse: 0.2339 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Training model with learning rate: 0.01\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 26ms/step - loss: 0.2338 - mse: 0.2338 - val_loss: 0.2350 - val_mse: 0.2350\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2340 - mse: 0.2340 - val_loss: 0.2350 - val_mse: 0.2350\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2335 - mse: 0.2335 - val_loss: 0.2350 - val_mse: 0.2350\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2334 - mse: 0.2334 - val_loss: 0.2352 - val_mse: 0.2352\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2351 - val_mse: 0.2351\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335 - val_loss: 0.2350 - val_mse: 0.2350\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2349 - val_mse: 0.2349\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335 - val_loss: 0.2351 - val_mse: 0.2351\n",
            "Training model with learning rate: 0.001\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 42ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2351 - val_mse: 0.2351\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2352 - val_mse: 0.2352\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2352 - val_mse: 0.2352\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2352 - val_mse: 0.2352\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2336 - mse: 0.2336 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Training model with learning rate: 0.0001\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 1s 30ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2337 - mse: 0.2337 - val_loss: 0.2353 - val_mse: 0.2353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSp9j4Pn-ZDt"
      },
      "source": [
        "### **Problem #3.2**\n",
        "\n",
        "To reach the full potential of smaller learning rates, we often need to compensate by running more epochs. So, modify the code below to increase the `epochs` parameter to `100` and find the ideal `learning_rate` parameter given everything else is fixed. Consider values such as: `10`, `1`, `0.1`, `0.01`, `0.001`, and `0.0001`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(30),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training model with learning rate: {lr}\")\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "    history = model.fit(X_train, y_train, epochs=100, batch_size=64)"
      ],
      "metadata": {
        "id": "sCRTeVWJ-ZD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b006e3d-d9b0-4501-a202-ea60ecd91d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with learning rate: 10\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 20806250496.0000 - mse: 20806250496.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 951.4104 - mse: 951.4104\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1723.8575 - mse: 1723.8575\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2161.0864 - mse: 2161.0864\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2382.7896 - mse: 2382.7896\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2487.3501 - mse: 2487.3501\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 2532.4480 - mse: 2532.4480\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2548.3628 - mse: 2548.3628\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2550.1399 - mse: 2550.1399\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2545.0261 - mse: 2545.0261\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2536.4583 - mse: 2536.4583\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2526.0447 - mse: 2526.0447\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2514.5398 - mse: 2514.5398\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2502.3040 - mse: 2502.3040\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2489.5046 - mse: 2489.5046\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2476.2351 - mse: 2476.2351\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2462.5522 - mse: 2462.5522\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2448.4832 - mse: 2448.4832\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2434.0522 - mse: 2434.0522\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2419.2808 - mse: 2419.2808\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2404.1770 - mse: 2404.1770\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2388.7676 - mse: 2388.7676\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2373.0635 - mse: 2373.0635\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2357.0825 - mse: 2357.0825\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2340.8362 - mse: 2340.8362\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2324.3286 - mse: 2324.3286\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2307.5818 - mse: 2307.5818\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2290.5981 - mse: 2290.5981\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2273.3943 - mse: 2273.3943\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2255.9810 - mse: 2255.9810\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2238.3728 - mse: 2238.3728\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2220.5686 - mse: 2220.5686\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2202.5886 - mse: 2202.5886\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2184.4380 - mse: 2184.4380\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2166.1299 - mse: 2166.1299\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2147.6726 - mse: 2147.6726\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2129.0681 - mse: 2129.0681\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2110.3352 - mse: 2110.3352\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2091.4697 - mse: 2091.4697\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2072.4907 - mse: 2072.4907\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2053.3997 - mse: 2053.3997\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2034.2080 - mse: 2034.2080\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 2014.9347 - mse: 2014.9347\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1995.5691 - mse: 1995.5691\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1976.1228 - mse: 1976.1228\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1956.6133 - mse: 1956.6133\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1937.0327 - mse: 1937.0327\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1917.4061 - mse: 1917.4061\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1897.7255 - mse: 1897.7255\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1878.0118 - mse: 1878.0118\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1858.2490 - mse: 1858.2490\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1838.4598 - mse: 1838.4598\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1818.6439 - mse: 1818.6439\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1798.8230 - mse: 1798.8230\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1778.9930 - mse: 1778.9930\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1759.1565 - mse: 1759.1565\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1739.3201 - mse: 1739.3201\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1719.4860 - mse: 1719.4860\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1699.6600 - mse: 1699.6600\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1679.8541 - mse: 1679.8541\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1660.0646 - mse: 1660.0646\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1640.3008 - mse: 1640.3008\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1620.5789 - mse: 1620.5789\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1600.8948 - mse: 1600.8948\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1581.2554 - mse: 1581.2554\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1561.6577 - mse: 1561.6577\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1542.1139 - mse: 1542.1139\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1522.6268 - mse: 1522.6268\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1503.1967 - mse: 1503.1967\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1483.8389 - mse: 1483.8389\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1464.5530 - mse: 1464.5530\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1445.3400 - mse: 1445.3400\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1426.1965 - mse: 1426.1965\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1407.1373 - mse: 1407.1373\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1388.1558 - mse: 1388.1558\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1369.2677 - mse: 1369.2677\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1350.4740 - mse: 1350.4740\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1331.7731 - mse: 1331.7731\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1313.1697 - mse: 1313.1697\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1294.6667 - mse: 1294.6667\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1276.2754 - mse: 1276.2754\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1257.9961 - mse: 1257.9961\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1239.8319 - mse: 1239.8319\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1221.7720 - mse: 1221.7720\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1203.8217 - mse: 1203.8217\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1185.9904 - mse: 1185.9904\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1168.2700 - mse: 1168.2700\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1150.6843 - mse: 1150.6843\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1133.2185 - mse: 1133.2185\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1115.8878 - mse: 1115.8878\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1098.6888 - mse: 1098.6888\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1081.6193 - mse: 1081.6193\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1064.6903 - mse: 1064.6903\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1047.8975 - mse: 1047.8975\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1031.2397 - mse: 1031.2397\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1014.7232 - mse: 1014.7232\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 998.3533 - mse: 998.3533\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 982.1124 - mse: 982.1124\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 966.0282 - mse: 966.0282\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 950.0878 - mse: 950.0878\n",
            "Training model with learning rate: 1\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 766.6729 - mse: 766.6729\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 396.2763 - mse: 396.2763\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 162.7162 - mse: 162.7162\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 45.6549 - mse: 45.6549\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 5.7294 - mse: 5.7294\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7266 - mse: 0.7266\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2.8156 - mse: 2.8156\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 2.7767 - mse: 2.7767\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.3499 - mse: 1.3499\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4221 - mse: 0.4221\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2469 - mse: 0.2469\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2751 - mse: 0.2751\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2724 - mse: 0.2724\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2460 - mse: 0.2460\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2350 - mse: 0.2350\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2350 - mse: 0.2350\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2328 - mse: 0.2328\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2365 - mse: 0.2365\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2385 - mse: 0.2385\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2327 - mse: 0.2327\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2377 - mse: 0.2377\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2352 - mse: 0.2352\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2352 - mse: 0.2352\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2357 - mse: 0.2357\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2354 - mse: 0.2354\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Training model with learning rate: 0.1\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 0.2425 - mse: 0.2425\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2381 - mse: 0.2381\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2386 - mse: 0.2386\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2422 - mse: 0.2422\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2371 - mse: 0.2371\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2397 - mse: 0.2397\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2379 - mse: 0.2379\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2384 - mse: 0.2384\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2359 - mse: 0.2359\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2366 - mse: 0.2366\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2349 - mse: 0.2349\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2373 - mse: 0.2373\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2359 - mse: 0.2359\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2429 - mse: 0.2429\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2419 - mse: 0.2419\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2385 - mse: 0.2385\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2390 - mse: 0.2390\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2351 - mse: 0.2351\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2356 - mse: 0.2356\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2356 - mse: 0.2356\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2380 - mse: 0.2380\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2402 - mse: 0.2402\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2350 - mse: 0.2350\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2370 - mse: 0.2370\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2375 - mse: 0.2375\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2439 - mse: 0.2439\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2349 - mse: 0.2349\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2441 - mse: 0.2441\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2350 - mse: 0.2350\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2357 - mse: 0.2357\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2381 - mse: 0.2381\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2359 - mse: 0.2359\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2476 - mse: 0.2476\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2438 - mse: 0.2438\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2381 - mse: 0.2381\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2365 - mse: 0.2365\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2407 - mse: 0.2407\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2405 - mse: 0.2405\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2376 - mse: 0.2376\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2388 - mse: 0.2388\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2384 - mse: 0.2384\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2417 - mse: 0.2417\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2420 - mse: 0.2420\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2487 - mse: 0.2487\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2369 - mse: 0.2369\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2371 - mse: 0.2371\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2459 - mse: 0.2459\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2378 - mse: 0.2378\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Training model with learning rate: 0.01\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2349 - mse: 0.2349\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2332 - mse: 0.2332\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2332 - mse: 0.2332\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2332 - mse: 0.2332\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Training model with learning rate: 0.001\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Training model with learning rate: 0.0001\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2335 - mse: 0.2335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzHIdT4P_1cw"
      },
      "source": [
        "### **Problem #3.3**\n",
        "\n",
        "The last main hyperparameter for us to consider is `batch_size`, which is how many data points we use to make a single update to the weights and parameters. Modify the code below to find the ideal `batch_size` given everything else is fixed. Consider values such as: `1`, `32`, `64`, `256`, and `len(X_train)`.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE**: Each epoch involves looking at *all* of the data points. In other words, **size of training data = batch_size * number of updates per epoch**. So, increasing `batch_size`, decreases the number of updates, which in turn speeds up each epoch. As such, pay attention to the amount of time it takes to run through each epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(30),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "batch_sizes = [1, 32, 64, 256, len(X_train)]\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"Training model with batch size: {batch_size}\")\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "6zuFps0I_1c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15222372-c374-4de5-8ae1-4524b3fcdcc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with batch size: 1\n",
            "Epoch 1/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 1181.8685 - mse: 1181.8685\n",
            "Epoch 2/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.3853 - mse: 0.3853\n",
            "Epoch 3/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.3287 - mse: 0.3287\n",
            "Epoch 4/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2829 - mse: 0.2829\n",
            "Epoch 5/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2542 - mse: 0.2542\n",
            "Epoch 6/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2401 - mse: 0.2401\n",
            "Epoch 7/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2352 - mse: 0.2352\n",
            "Epoch 8/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 9/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 10/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 11/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 12/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 13/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 14/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 15/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2352 - mse: 0.2352\n",
            "Epoch 16/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 17/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 18/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 19/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2349 - mse: 0.2349\n",
            "Epoch 20/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2371 - mse: 0.2371\n",
            "Epoch 21/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2383 - mse: 0.2383\n",
            "Epoch 22/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2363 - mse: 0.2363\n",
            "Epoch 23/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 24/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2365 - mse: 0.2365\n",
            "Epoch 25/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 26/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2366 - mse: 0.2366\n",
            "Epoch 27/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2365 - mse: 0.2365\n",
            "Epoch 28/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 29/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2352 - mse: 0.2352\n",
            "Epoch 30/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2361 - mse: 0.2361\n",
            "Epoch 31/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2361 - mse: 0.2361\n",
            "Epoch 32/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2366 - mse: 0.2366\n",
            "Epoch 33/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 34/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 35/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 36/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 37/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2378 - mse: 0.2378\n",
            "Epoch 38/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2367 - mse: 0.2367\n",
            "Epoch 39/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2378 - mse: 0.2378\n",
            "Epoch 40/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2357 - mse: 0.2357\n",
            "Epoch 41/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 42/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2372 - mse: 0.2372\n",
            "Epoch 43/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2365 - mse: 0.2365\n",
            "Epoch 44/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2363 - mse: 0.2363\n",
            "Epoch 45/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2351 - mse: 0.2351\n",
            "Epoch 46/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2372 - mse: 0.2372\n",
            "Epoch 47/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 48/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2357 - mse: 0.2357\n",
            "Epoch 49/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2370 - mse: 0.2370\n",
            "Epoch 50/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 51/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2361 - mse: 0.2361\n",
            "Epoch 52/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Epoch 53/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2364 - mse: 0.2364\n",
            "Epoch 54/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2367 - mse: 0.2367\n",
            "Epoch 55/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2367 - mse: 0.2367\n",
            "Epoch 56/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 57/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2361 - mse: 0.2361\n",
            "Epoch 58/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 59/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 60/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2356 - mse: 0.2356\n",
            "Epoch 61/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 62/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 63/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 64/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2386 - mse: 0.2386\n",
            "Epoch 65/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2375 - mse: 0.2375\n",
            "Epoch 66/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2375 - mse: 0.2375\n",
            "Epoch 67/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2368 - mse: 0.2368\n",
            "Epoch 68/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2368 - mse: 0.2368\n",
            "Epoch 69/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2357 - mse: 0.2357\n",
            "Epoch 70/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 71/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2369 - mse: 0.2369\n",
            "Epoch 72/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2370 - mse: 0.2370\n",
            "Epoch 73/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2362 - mse: 0.2362\n",
            "Epoch 74/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2381 - mse: 0.2381\n",
            "Epoch 75/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2375 - mse: 0.2375\n",
            "Epoch 76/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 77/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2364 - mse: 0.2364\n",
            "Epoch 78/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 79/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2367 - mse: 0.2367\n",
            "Epoch 80/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2355 - mse: 0.2355\n",
            "Epoch 81/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2380 - mse: 0.2380\n",
            "Epoch 82/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2388 - mse: 0.2388\n",
            "Epoch 83/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2366 - mse: 0.2366\n",
            "Epoch 84/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2377 - mse: 0.2377\n",
            "Epoch 85/100\n",
            "455/455 [==============================] - 1s 1ms/step - loss: 0.2368 - mse: 0.2368\n",
            "Epoch 86/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2364 - mse: 0.2364\n",
            "Epoch 87/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 88/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2364 - mse: 0.2364\n",
            "Epoch 89/100\n",
            "455/455 [==============================] - 1s 3ms/step - loss: 0.2351 - mse: 0.2351\n",
            "Epoch 90/100\n",
            "455/455 [==============================] - 1s 3ms/step - loss: 0.2375 - mse: 0.2375\n",
            "Epoch 91/100\n",
            "455/455 [==============================] - 2s 4ms/step - loss: 0.2376 - mse: 0.2376\n",
            "Epoch 92/100\n",
            "455/455 [==============================] - 2s 4ms/step - loss: 0.2383 - mse: 0.2383\n",
            "Epoch 93/100\n",
            "455/455 [==============================] - 2s 5ms/step - loss: 0.2359 - mse: 0.2359\n",
            "Epoch 94/100\n",
            "455/455 [==============================] - 2s 3ms/step - loss: 0.2360 - mse: 0.2360\n",
            "Epoch 95/100\n",
            "455/455 [==============================] - 1s 3ms/step - loss: 0.2370 - mse: 0.2370\n",
            "Epoch 96/100\n",
            "455/455 [==============================] - 2s 3ms/step - loss: 0.2363 - mse: 0.2363\n",
            "Epoch 97/100\n",
            "455/455 [==============================] - 1s 3ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 98/100\n",
            "455/455 [==============================] - 1s 3ms/step - loss: 0.2380 - mse: 0.2380\n",
            "Epoch 99/100\n",
            "455/455 [==============================] - 2s 4ms/step - loss: 0.2367 - mse: 0.2367\n",
            "Epoch 100/100\n",
            "455/455 [==============================] - 1s 2ms/step - loss: 0.2353 - mse: 0.2353\n",
            "Training model with batch size: 32\n",
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 3ms/step - loss: 0.2354 - mse: 0.2354\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2349 - mse: 0.2349\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2358 - mse: 0.2358\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Training model with batch size: 64\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2341 - mse: 0.2341\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2345 - mse: 0.2345\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2333 - mse: 0.2333\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2346 - mse: 0.2346\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2348 - mse: 0.2348\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2344 - mse: 0.2344\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2338 - mse: 0.2338\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2332 - mse: 0.2332\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - mse: 0.2342\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2339 - mse: 0.2339\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2340 - mse: 0.2340\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2343 - mse: 0.2343\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Training model with batch size: 256\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 1s 7ms/step - loss: 0.2334 - mse: 0.2334\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2337 - mse: 0.2337\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2336 - mse: 0.2336\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Training model with batch size: 455\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - mse: 0.2335\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2335 - mse: 0.2335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3.4**\n",
        "\n",
        "With so many hyperparameters to tune and potentially deep & complex NNs, the amount of information printed out often becomes less helpful to dig through. It is common practice to visualize the performance of different models for each epoch instead. Furthermore, we can \"mute\" the output of the `fit(...)` function by setting `verbose = False`.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Complete the code below to train and visualize the losses of a NN with the best set of hyperparameters you found above.**\n",
        "\n",
        "\n",
        "Some questions to consider:\n",
        "* Does the loss tend to increase or decrease?\n",
        "* Should we have let the model train for more epochs?\n",
        "* Could we have gotten away with fewer epochs?\n"
      ],
      "metadata": {
        "id": "a_J7TGOlvPBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(30))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, verbose=False)\n",
        "\n",
        "loss = history.history['loss']\n",
        "plt.plot(loss)\n",
        "\n",
        "plt.title('Loss After Each Epoch', fontsize='x-large')\n",
        "plt.xlabel('Epoch', fontsize='x-large')\n",
        "plt.ylabel('Loss', fontsize='x-large')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "62uQBW52D9mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "07233de1-d7a1-47ae-9850-c5a8ca02fdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##© 2024 The Coding School, All rights reserved"
      ],
      "metadata": {
        "id": "21uTwbSQMZnJ"
      }
    }
  ]
}