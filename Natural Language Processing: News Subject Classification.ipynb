{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7dzC09dLlEhm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-guan/Python-DataSci-ML/blob/main/Natural%20Language%20Processing%3A%20News%20Subject%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 20: Natural Language Processing I**\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "In this week's lab, we will see how to use neural networks for one of the most popular NLP tasks: **text classification**. This will involve applying what you already know about neural nets and new NLP concepts of tokenization and vectorization.\n",
        "\n",
        "For this project, we will be working with the `fetch_20newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Each newsgroup covers a different topic, such as sports, politics, religion, and technology. The documents within each newsgroup were posted by various authors, and cover a wide range of subtopics related to the main theme of the newsgroup.\n",
        "\n",
        "The goal of this project is to build a machine learning model that can accurately classify newsgroup documents based on their content.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Lab Structure**\n",
        "**Part 1**: [News Subject Classification with a DNN](#p1)\n",
        "\n",
        "**Part 2**: [News Subject Classification with a CNN](#p2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Goals**\n",
        "By the end of this lab, you will:\n",
        "* Understand the concept of tokenization in NLP.\n",
        "* Compare a fully connected network to a CNN for text classification.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Cheat Sheets**\n",
        "[Natural Language Processing I](https://docs.google.com/document/d/1ZaLtMF7aQsG05myetJpoTJlr-sAIURP_a9sQr66pfqw/edit?usp=drive_link)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**\n"
      ],
      "metadata": {
        "id": "mbZXQ3rA3NwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch==1.13.1\n",
        "!pip install --quiet torchdata==0.5.1\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchtext\n",
        "import torchtext\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "YAvvLhRIoqYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc97559-b1b1-4b39-da21-ccd6851b91f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.17.1 requires torchdata==0.7.1, but you have torchdata 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.43.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Collecting torch==2.2.1 (from torchtext)\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Collecting torchdata==0.7.1 (from torchtext)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.5.1\n",
            "    Uninstalling torchdata-0.5.1:\n",
            "      Successfully uninstalled torchdata-0.5.1\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 torchdata-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Part 1: News Subject Classification with a DNN**\n",
        "---\n",
        "\n",
        "In this section, we will learn how to tokenize the results of a neural network classifying news articles by subject. We will see how to use keras's `TextVectorization` layer alongside the other layers we have seen to classify these articles.\n",
        "\n",
        "We will be working with the [AG News dataset](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html), which is a corpus of over 1 million articles from more than 2000 sources and is commonly used in academic research (see its [papers with code page](https://paperswithcode.com/dataset/ag-news) for more information).\n",
        "\n",
        "<br>\n",
        "\n",
        "Each input will be text containing an article's title, source, and a snippet from the article itself. We will then classify each input as one of these subjects: `\"World\"`, `\"Sports\"`, `\"Business\"`, or `\"Sci/Tech\"`.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Run the code provided below to import the dataset.**"
      ],
      "metadata": {
        "id": "idga37M2FsMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n",
        "\n",
        "x_train, y_train = [], []\n",
        "for Y, X in train_dataset:\n",
        "    x_train.append(X)\n",
        "    y_train.append(Y)\n",
        "\n",
        "x_test, y_test = [], []\n",
        "for Y, X in test_dataset:\n",
        "    x_test.append(X)\n",
        "    y_test.append(Y)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "y_train, y_test = np.array(y_train) - 1, np.array(y_test) - 1\n",
        "y_train = to_categorical(y_train, dtype = 'int32')\n",
        "y_test = to_categorical(y_test, dtype = 'int32')"
      ],
      "metadata": {
        "id": "In9BSfqd_OPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.1: Create the `TextVectorization` layer**\n",
        "\n",
        "Let's create a `TextVectorization` layer to vectorize this data.\n",
        "\n",
        "Specifically,\n",
        "1. Initialize the layer with the specified parameters.\n",
        "\n",
        "2. Adapt the layer to the training data."
      ],
      "metadata": {
        "id": "E90o2LJcwsMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Initialize the layer with the specified parameters.**\n",
        "\n",
        "* `max_tokens = 5000`\n",
        "* `output_mode = 'int'`\n",
        "* `output_sequence_length = 50`"
      ],
      "metadata": {
        "id": "koGBxiapJC_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = 5000,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 50\n",
        "  )"
      ],
      "metadata": {
        "id": "sXTATRLVm2Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***STOP!* Answer the following question: Why does every output sequence need to be the same length?**"
      ],
      "metadata": {
        "id": "qMFXTuDwvJxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. Adapt the layer to the training data.**"
      ],
      "metadata": {
        "id": "wHH-kKo9pi7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "36mXm5tkpwLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.2: Look at the vocabulary**\n",
        "\n",
        "\n",
        "**Run the code below to look at a portion of the vocabulary that was just built for the training data.**"
      ],
      "metadata": {
        "id": "Vz2dKD8pJhXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()[0:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA6CEFVtDeQa",
        "outputId": "d6d3fce8-1780-4791-f5e9-21df4d3643ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'to',\n",
              " 'a',\n",
              " 'of',\n",
              " 'in',\n",
              " 'and',\n",
              " 'on',\n",
              " 'for',\n",
              " '39s',\n",
              " 'that',\n",
              " 'with',\n",
              " 'as',\n",
              " 'at',\n",
              " 'its',\n",
              " 'is',\n",
              " 'new',\n",
              " 'by',\n",
              " 'said',\n",
              " 'it',\n",
              " 'us',\n",
              " 'has',\n",
              " 'from',\n",
              " 'reuters',\n",
              " 'an',\n",
              " 'ap',\n",
              " 'his',\n",
              " 'will',\n",
              " 'after',\n",
              " 'was',\n",
              " 'be',\n",
              " 'over',\n",
              " 'have',\n",
              " 'their',\n",
              " 'are',\n",
              " 'up',\n",
              " 'but',\n",
              " 'first',\n",
              " 'more',\n",
              " 'two',\n",
              " 'he',\n",
              " 'this',\n",
              " 'world',\n",
              " 'monday',\n",
              " 'wednesday',\n",
              " 'tuesday',\n",
              " 'oil',\n",
              " 'out',\n",
              " 'thursday']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.3: Add the input and text vectorization layers to the model**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zTCj-Z7rJ5on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)"
      ],
      "metadata": {
        "id": "RYh5A78OKRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.4: Look at the vectorization of an example**\n",
        "\n",
        "\n",
        "Add your own sentence below to see how it would be vectorized with our newly adapted layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** `TextVectorizer` will ignore any punctuation and consider upper and lower case the same. There are extra parameters that can set to adjust this."
      ],
      "metadata": {
        "id": "Ohs_ludzKcE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_0 = model.predict([\"Go Bruins!\"])\n",
        "\n",
        "print(vector_0)\n",
        "print(vector_0.shape)"
      ],
      "metadata": {
        "id": "PZoit5S6OnPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f27f61-34d6-4991-f459-235c9b9ce93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 265ms/step\n",
            "[[342   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "(1, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***STOP!* Answer the following question: What does each number in the output vector represent?**"
      ],
      "metadata": {
        "id": "iNEGQr1YumaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.5: Add hidden layers and an output layer**\n",
        "\n",
        "\n",
        "Add two dense layers with 512 neurons and ReLU activation.\n",
        "\n",
        "Then, create the output layer so we can classify the data as `\"World\"`, `\"Sports\"`, `\"Business\"`, or `\"Sci/Tech\"`. You will use the softmax activation function."
      ],
      "metadata": {
        "id": "RbqWrZRxm2Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(units=4, activation='softmax'))"
      ],
      "metadata": {
        "id": "c3yMuWf1m2Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our DNN."
      ],
      "metadata": {
        "id": "zYru7HAFGz3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI8Yhy61G2Ar",
        "outputId": "ee37369c-f946-4c8f-b4d7-ec376ad963e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (Text  (None, 50)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               26112     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816132 (3.11 MB)\n",
            "Trainable params: 816132 (3.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.6: Compile and fit the model**\n",
        "\n",
        "\n",
        "Compile and fit the model with the following parameters:\n",
        "* Adam learning rate of 0.001\n",
        "* `categorical_crossentropy` for the loss function\n",
        "* Accuracy as the metric\n",
        "* For the fit, use `epochs=5` and `batch_size=256`"
      ],
      "metadata": {
        "id": "EI8BNJxFm2Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=256)"
      ],
      "metadata": {
        "id": "rauKiC4ym2Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e172e42a-cdee-41e3-8bde-b08ac74cad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 50s 39ms/step - loss: 1.3868 - accuracy: 0.2495\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 1.3869 - accuracy: 0.2488\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 1.3868 - accuracy: 0.2477\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 1.3867 - accuracy: 0.2506\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 1.3867 - accuracy: 0.2489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d8160ae7bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #1.7: Evaluate the model**\n",
        "\n",
        "\n",
        "Now, evaluate the model for both the training and test sets.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** As a baseline, randomly guessing 1 out of 4 possible classes would achieve a roughly 0.25 accuracy."
      ],
      "metadata": {
        "id": "9Brgtf8cm2Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the training set\n",
        "model.evaluate(x_train, y_train)\n",
        "\n",
        "# Evaluate the test set\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "VpcaQMrfm2Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1f7af5-bbe1-4bce-e3cd-4dff73418235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3750/3750 [==============================] - 22s 6ms/step - loss: 1.3865 - accuracy: 0.2500\n",
            "238/238 [==============================] - 1s 5ms/step - loss: 1.3865 - accuracy: 0.2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.386467695236206, 0.25]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<center>\n",
        "\n",
        "### **Back to lecture**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9l2YAKbUv7JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Part 2: News Subject Classification with a CNN**\n",
        "---\n",
        "\n",
        "The model in Part 1 likely did not do much better than random guessing. Let's try with a CNN instead."
      ],
      "metadata": {
        "id": "q-mvo-kmm2Mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by building a new CNN model. Remember, the syntax for CNNs for NLP is a little different than for images. We will be using the 1D versions of the convolution and max pooling layers. Examples:\n",
        "* `Conv1D(filters=128, kernel_size=5, activation='relu')`\n",
        "* `MaxPooling1D(pool_size=2)`\n"
      ],
      "metadata": {
        "id": "gyAN7zkgpRRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #2.1: Initialize the model with an input and vectorizer layer**\n",
        "\n",
        "\n",
        "*Hint: This is the same as last time.*"
      ],
      "metadata": {
        "id": "VN8Z0gy_sExR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "\n",
        "cnn_model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
        "cnn_model.add(vectorize_layer)"
      ],
      "metadata": {
        "id": "LvS3JpkcsZI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #2.2: Finish building the CNN**\n",
        "\n",
        "\n",
        "Build your CNN with the following layers:\n",
        "* a convolutional layer with 64 filters, a kernel size of 5, and ReLU activation\n",
        "* a max pooling layer with a pool size of 2\n",
        "* a convolutional layer with 128 filters, a kernel size of 5, and ReLU activation\n",
        "* a max pooling layer with a pool size of 2\n",
        "* a flatten layer\n",
        "* a dense layer with 256 neurons\n",
        "* the output layer"
      ],
      "metadata": {
        "id": "9kV4xruit_UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The convolution layer requires us to cast the inputs to a different data type\n",
        "# and reshape the input as well. We have done this for you.\n",
        "cnn_model.add(Lambda(lambda x: tf.cast(x, 'float32')))\n",
        "cnn_model.add(Reshape((50, 1)))\n",
        "\n",
        "# Start building your CNN below.\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(256, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "cnn_model.add(Dense(4, activation='softmax'))"
      ],
      "metadata": {
        "id": "8SQd40rbnOrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the completed model."
      ],
      "metadata": {
        "id": "VfFAaOAOizPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqK1H52pi1iZ",
        "outputId": "046ee88b-4cf8-401b-f11f-e66417febc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (Text  (None, 50)                0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " lambda_6 (Lambda)           (None, 50)                0         \n",
            "                                                                 \n",
            " reshape_6 (Reshape)         (None, 50, 1)             0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 46, 64)            384       \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPoolin  (None, 23, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 19, 128)           41088     \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 9, 128)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               295168    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 4)                 8         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 336905 (1.29 MB)\n",
            "Trainable params: 336905 (1.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #2.3: Compile and fit the model**\n",
        "\n",
        "\n",
        "Compile and fit the model with the same parameters as Part 1."
      ],
      "metadata": {
        "id": "kRhmTL_2lOM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(x_train, y_train, epochs=5, batch_size=256)"
      ],
      "metadata": {
        "id": "RKS-Rxcsla4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7669d50-401a-4b4d-8f96-ad4b1512ad06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 32s 65ms/step - loss: 0.6284 - accuracy: 0.2501\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 33s 71ms/step - loss: 0.5739 - accuracy: 0.2500\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 30s 63ms/step - loss: 0.5638 - accuracy: 0.2500\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 30s 63ms/step - loss: 0.5625 - accuracy: 0.2500\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 31s 66ms/step - loss: 0.5623 - accuracy: 0.2503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d815363a5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problem #2.4: Evaluate the model**\n",
        "\n",
        "\n",
        "Now, evaluate the model for both the training and test sets."
      ],
      "metadata": {
        "id": "FTIEF_GHlGZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.evaluate(x_train, y_train)\n",
        "cnn_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "1VJ42ZnfloOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99eec56-9bb7-4ce0-b79d-dbec647c66d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3750/3750 [==============================] - 24s 6ms/step - loss: 0.5623 - accuracy: 0.2500\n",
            "238/238 [==============================] - 1s 5ms/step - loss: 0.5623 - accuracy: 0.2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5623378753662109, 0.25]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oh no!** It looks like the CNN didn't do much better! It turns out that tokenization and vectorization is not enough to prepare text data for deep learning. There's an additional processing step we can take that will set our models up for success: **embedding.** We will see how embedding improves model performance in next week's lab."
      ],
      "metadata": {
        "id": "CGszeqCvm2Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of notebook\n",
        "---\n",
        "© 2024 The Coding School, All rights reserved"
      ],
      "metadata": {
        "id": "7dzC09dLlEhm"
      }
    }
  ]
}