{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-guan/Python-DataSci-ML/blob/main/Natural%20Language%20Processing%3A%20Sentiment%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 22: Natural Language Processing Review**\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "In this week's homework, you will review how to use more advanced forms of neural nets to perform tasks in NLP such as classification.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Structure**\n",
        "**Part 1**: IMDB Sentiment Classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Cheat Sheets**\n",
        "[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**"
      ],
      "metadata": {
        "id": "dTIoQdGufbcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from random import choices\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yWd0JEE9fmrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"p1\"></a>\n",
        "\n",
        "---\n",
        "## **Part 1: IMDB Sentiment Classification**\n",
        "---\n",
        "\n",
        "In this part we will focus on building a CNN model using the IMDB sentiment classification dataset. This is a dataset of 25,000 movie reviews with sentiment labels: 0 for negative and 1 for positive.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Run the code provided below to import the dataset.**"
      ],
      "metadata": {
        "id": "Tnnv9dfa0I-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTdgncgNHtppfS89LHOh1kGl5tYzoEUrUwmOPOQF7mQ0U5Rzba27H45imvZ06_J2x0-wCJySylP5V3_/pub?gid=1712575053&single=true&output=csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size = 0.2, random_state = 42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "ACX4sfJW0bq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.1: Create the `TextVectorization` layer**\n",
        "\n",
        "\n",
        "To get started, let's create a `TextVectorization` layer to vectorize this data.\n",
        "\n",
        "Specifically,\n",
        "1. Initialize the layer with the specified parameters.\n",
        "\n",
        "2. Adapt the layer to the training data.\n",
        "\n",
        "3. Look at the newly built vocabulary."
      ],
      "metadata": {
        "id": "HuGSwhDc0e1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Initialize the layer with the specified parameters.**\n",
        "\n",
        "* The vocabulary should be at most 5000 words.\n",
        "* The layer's output should always be 64 integers."
      ],
      "metadata": {
        "id": "JlbOhj9L0hdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = 5000,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 64\n",
        "  )"
      ],
      "metadata": {
        "id": "r4CA7F9W0l4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Adapt the layer to the training data.**"
      ],
      "metadata": {
        "id": "3SgPJHmy0plM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "sGMMXLG50ruI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Look at the newly built vocabulary.**"
      ],
      "metadata": {
        "id": "7KWwtA--0ue-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "t_7qWqMl0xAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b519bee8-26c0-4532-87c0-003cc5f160b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'this',\n",
              " 'that',\n",
              " 'br',\n",
              " 'was',\n",
              " 'as',\n",
              " 'for',\n",
              " 'with',\n",
              " 'movie',\n",
              " 'but',\n",
              " 'film',\n",
              " 'on',\n",
              " 'not',\n",
              " 'you',\n",
              " 'are',\n",
              " 'his',\n",
              " 'have',\n",
              " 'be',\n",
              " 'he',\n",
              " 'one',\n",
              " 'its',\n",
              " 'at',\n",
              " 'all',\n",
              " 'by',\n",
              " 'an',\n",
              " 'they',\n",
              " 'from',\n",
              " 'who',\n",
              " 'so',\n",
              " 'like',\n",
              " 'just',\n",
              " 'or',\n",
              " 'her',\n",
              " 'about',\n",
              " 'if',\n",
              " 'has',\n",
              " 'out',\n",
              " 'some',\n",
              " 'there',\n",
              " 'what']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.2: Build and Train a Dense model**\n",
        "\n",
        "Complete the code below to build a model with the following layers.\n",
        "\n",
        "An Embedding layer such that:\n",
        "- The vocabulary contains 5000 tokens.\n",
        "- The input length corresponds to the output of the vectorization layer.\n",
        "- The number of outputs per input is 128.\n",
        "\n",
        "<br>\n",
        "\n",
        "Hidden layers such that:\n",
        "\n",
        "- There's at least one Dense layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "A Dense layer for outputting classification probabilities for \"negative\" or \"positive\" labels."
      ],
      "metadata": {
        "id": "XH9dVdBW1D4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "tr-vmrCI1_mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This other alternative includes building the model with CNN.\n",
        "**Which architecture performs better?**"
      ],
      "metadata": {
        "id": "_UHdf2m15v1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [OPTIONAL] USING CNNs\n",
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Conv1D(filters = 16, kernel_size = 4, activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = 3))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Printing Structure\n",
        "for layer in model.layers:\n",
        "  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Fitting\n",
        "opt = Adam(learning_rate = 0.001)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n",
        "\n",
        "\n",
        "# Evaluating\n",
        "print(\"\\n\\n\\n\")\n",
        "model.evaluate(x_train, y_train)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSM9qX412zyL",
        "outputId": "c1533407-099d-44cd-fe2b-e341f5af7c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1) -> (None, 64)\n",
            "(None, 64) -> (None, 64, 128)\n",
            "(None, 64, 128) -> (None, 61, 16)\n",
            "(None, 61, 16) -> (None, 20, 16)\n",
            "(None, 20, 16) -> (None, 320)\n",
            "(None, 320) -> (None, 2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 19s 88ms/step - loss: 0.5690 - accuracy: 0.6965\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 6s 40ms/step - loss: 0.3868 - accuracy: 0.8254\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 6s 35ms/step - loss: 0.3266 - accuracy: 0.8593\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 4s 23ms/step - loss: 0.2583 - accuracy: 0.8985\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 3s 20ms/step - loss: 0.1763 - accuracy: 0.9457\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 0.1024 - accuracy: 0.9844\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.5172 - accuracy: 0.7811\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5171982645988464, 0.7810999751091003]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dzC09dLlEhm"
      },
      "source": [
        "---\n",
        "#End of notebook\n",
        "\n",
        "Â© 2024 The Coding School, All rights reserved"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}