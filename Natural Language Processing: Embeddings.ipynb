{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessica-guan/Python-DataSci-ML/blob/main/Natural%20Language%20Processing%3A%20Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 21: Natural Language Processing II**\n",
        "---\n",
        "\n",
        "### **Description**\n",
        "In today's lab, we have another text classification task, but this time we will be using **embeddings.** For this project, we will be working with a dataset of BBC News articles classified by topic.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Lab Structure**\n",
        "\n",
        "**Part 1**: [Text Classification of BBC Articles](#p1)\n",
        "\n",
        "**Part 2**: [Convolutional Neural Networks](#p2)\n",
        "\n",
        "\n",
        "**Part 3**: [[OPTIONAL] IMDB Sentiment Classification](#p3)\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Goals**\n",
        "By the end of this lab, you will:\n",
        "* Understand how to apply embedding layers in models.\n",
        "* Compare a fully connected network to a CNN for text classification with embeddings.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Cheat Sheets**\n",
        "[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**"
      ],
      "metadata": {
        "id": "3u6a3u2FYvkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n",
        "\n",
        "from lime import lime_text\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from random import choices\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yWd0JEE9fmrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f36165-8328-4366-f2b9-2040123030cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=a410cf3425f24438edabfbfb2f224c6ee64b827827ccd787da183a8172564bc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"p1\"></a>\n",
        "\n",
        "---\n",
        "## **Part 1: Text Classification of BBC Articles**\n",
        "---\n",
        "\n",
        "In this section, we'll apply our text classification knowledge to a corpus of BBC articles. Your task is to develop a model that categorizes articles based on snippets of text, assigning each to a specific category. Unlike previous labs where we focused on visual data, here we'll use neural networks with traditional Dense layers to process and classify text data.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "**Run the code provided below to import the dataset.**"
      ],
      "metadata": {
        "id": "zuSSFz900B2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRRiQ1DUkUxk31YpaHA2i9QtwGq_VGXiy86z7l3aT9v5zoB6M7a-2M2qlYckr1C_ZG6StBELlU_hD3S/pub?output=csv')"
      ],
      "metadata": {
        "id": "U2ek2hb10flg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.1: Determine the number of categories**\n",
        "\n",
        "\n",
        "Using any necessary pandas functions or attributes, determine the total number of unique categories that the texts are assigned to.\n",
        "\n",
        "**In the cell code below, display the first few rows in the dataset.**\n"
      ],
      "metadata": {
        "id": "W4UR3nMoySPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "9bL0s5LH4mB5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5b9245b7-e042-47f7-f7ab-135eb655c0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  category  category_id\n",
              "0  libya takes 1bn unfrozen funds libya withdrawn...  business            0\n",
              "1  saudi investor picks savoy famous savoy hotel ...  business            0\n",
              "2  tate lyle boss bags award tate lyle chief exec...  business            0\n",
              "3  uk economy facing major risks uk manufacturing...  business            0\n",
              "4  aids climate davos agenda climate change fight...  business            0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16761b35-9b92-4d9a-8038-35e168366993\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>libya takes 1bn unfrozen funds libya withdrawn...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>saudi investor picks savoy famous savoy hotel ...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tate lyle boss bags award tate lyle chief exec...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uk economy facing major risks uk manufacturing...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aids climate davos agenda climate change fight...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16761b35-9b92-4d9a-8038-35e168366993')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-16761b35-9b92-4d9a-8038-35e168366993 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-16761b35-9b92-4d9a-8038-35e168366993');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac52c090-a3bd-44ce-8901-107f75faf765\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac52c090-a3bd-44ce-8901-107f75faf765')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac52c090-a3bd-44ce-8901-107f75faf765 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 2225,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2119,\n        \"samples\": [\n          \"web logs aid disaster recovery vivid descriptions devastation southern asia internet web logs blogs bloggers offering snapshots information region providing useful information want help indian writer rohit gupta edits group blog dogs borders created site supposed forum discuss relations india pakistan wake tsunami mr gupta fellow bloggers switched gears wanted blog tsunami aftermath sri lankan blogger group goes online morquendi internet service disrupted tsunami morquendi started sending sms text messages cell phone affected areas sri lanka started publishing smses says mr gupta morquendi describing scenes 600 bodies washed shore people burying burying burying people digging holes hands sms message didn t visual accounts print soon thousands web world logging read morquendi hand accounts message morquendi wrote sri lankan woman running friend wave swept away morquendi message read grabbed tree hand friend says watched water pull friend away mr gupta says power morquendi text message blogs palpable running friends burying bodies carrying bodies mr gupta says morquendi t begin imagine psychological sending reports relief work caught journalist human blogs helping spread information relief efforts dina mehta indian blogger helping newly created east asia earthquake tsunami blog says blog meant filled person accounts building resource says says ok want come work india volunteer india sri lanka malaysia sort stop shop come sorts resources emergency help lines relief agencies aid agencies contacts etc ms mehta says wishes governments region realise power blogs imagine resource available disaster quickly funnel aid people help says bloggers united states involved ramdhan yadav kotamaraja originally india lives dallas mr kotamaraja wanted help affected tsunami pooling concerned friends online payment says mr kotamaraja blogging world blogger friends started linking site saw people friends d 70 donations people don t know simply unbelievable people don t know come donating spreads quickly weblogs phenomenon helps bloggers expand audience scope sri lanka blogger morquendi recruiting help recruit calls heretic posts heretic asks seen fishing trawlers road seen bus inside heretic writes affected areas imagine concludes blogged clark boyd correspondent world world service wgbh boston co production\",\n          \"profits slide india dr reddy profits indian drugmaker dr reddy fell 93 research costs rose sales flagged firm profits 40 m rupees 915 000 486 000 months december sales fell 8 7bn rupees dr reddy built reputation producing generic versions big pharmaceutical products competition intensified firm short new product launches annoucement december 2000 won exclusive marketing rights generic version famous anti depressant prozac maker eli lilly key court case march banning selling version pfizer popular hypertension drug norvasc research development new drugs continuing apace r d spending rising 37 705 m rupees key cause decrease profits alongside fall sales patents known products near future representing opportunity dr reddy shares listed new york indian generics manufacturers sales dr reddy generics business fell 8 966 m rupees staple firm business sale ingredients drugs performed poorly sales 25 previous 4bn rupees face strong competition dr reddy indian competitors gathering strength face heavy competitive pressures\",\n          \"elvis fans hold birthday bash elvis fans marking legendary singer 70th birthday saturday day elvis convention took place blackpool england weekend aim finding elvis impersonator graceland tennessee home focus celebrations days events including concert memphis symphony orchestra elvis single jailhouse rock uk number sunday fans france celebrated tribute concert elvis cover bands special exhibition memorabilia display bonn germany jailhouse rock 999th number single uk pop history sonybmg releasing elvis 18 number singles rate week complete original artwork collector box hit single night follow week chance 000th number interest surrounding elvis birthday grows hmv spokesman gennaro castaldo fantastic truly fitting celebrate elvis landmark birthday\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"entertainment\",\n          \"tech\",\n          \"politics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see which of the categories in our dataset are unique."
      ],
      "metadata": {
        "id": "VIO4pIXXI846"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"category\"].unique())\n",
        "print(len(dataset[\"category\"].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1KvWpjI4noW",
        "outputId": "0ebd1131-5c52-49f2-a8c0-5d000eb53165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['business' 'entertainment' 'politics' 'sport' 'tech']\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** How many unique categories do we have from the above output? What are the unique categories?"
      ],
      "metadata": {
        "id": "JcZ388FPJFk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.2: Split the data into training and test sets**\n",
        "\n",
        "\n",
        "Determine the correct variables to use as the feature(s) and label here, making sure to provide numerical labels for the neural network to predict."
      ],
      "metadata": {
        "id": "NvqXQFlj4bl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features and labels\n",
        "x = dataset['text'].values\n",
        "y = dataset['category_id'].values\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "Pe4Cg-y9y7WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.3: Create the `TextVectorization` layer**\n",
        "\n",
        "\n",
        "To get started building the neural network, create a `TextVectorization` layer to vectorize this data.\n",
        "\n",
        "Specifically,\n",
        "1. Initialize the layer with the specified parameters.\n",
        "\n",
        "2. Adapt the layer to the training data.\n",
        "\n",
        "3. Look at the newly built vocabulary."
      ],
      "metadata": {
        "id": "LTm3147qiKE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.Initialize the layer with the specified parameters.**\n",
        "\n",
        "* The vocabulary should be at most 10000 words.\n",
        "* The layer's output should always be 20 integers."
      ],
      "metadata": {
        "id": "d3zfKfgAiKE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = 10000,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 20\n",
        "  )"
      ],
      "metadata": {
        "id": "0TUaQw3L7W1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Adapt the layer to the training data.**"
      ],
      "metadata": {
        "id": "Bif_POk3iKE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "bsnDnHGFiKE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Look at the newly built vocabulary.**"
      ],
      "metadata": {
        "id": "ovHm-Icy2iZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "bnAtuO3B2iZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb767f79-9b23-4972-8cd8-9d7871134529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'mr',\n",
              " 'm',\n",
              " 'people',\n",
              " 'new',\n",
              " '£',\n",
              " 't',\n",
              " 'government',\n",
              " 'film',\n",
              " 'year',\n",
              " 'uk',\n",
              " 'music',\n",
              " 'game',\n",
              " 'world',\n",
              " 'best',\n",
              " 'labour',\n",
              " 'election',\n",
              " 'time',\n",
              " 'blair',\n",
              " '1',\n",
              " 'party',\n",
              " 'games',\n",
              " 'mobile',\n",
              " 'market',\n",
              " 'england',\n",
              " '000',\n",
              " 'tv',\n",
              " '3',\n",
              " '2',\n",
              " 'sales',\n",
              " 'like',\n",
              " 'years',\n",
              " 'told',\n",
              " 'home',\n",
              " 'million',\n",
              " 'company',\n",
              " 'bbc',\n",
              " 'says',\n",
              " 'play',\n",
              " 'technology',\n",
              " '6',\n",
              " 'tax',\n",
              " 'net',\n",
              " 'firm',\n",
              " 'win',\n",
              " 'economy',\n",
              " 'digital',\n",
              " 'brown',\n",
              " 'number']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.4: Build the model**\n",
        "\n",
        "\n",
        "Complete the code below to build a model with the following layers.\n",
        "\n",
        "An `Embedding` layer such that:\n",
        "* The vocabulary contains 10000 tokens.\n",
        "* The input length corresponds to the output of the vectorization layer.\n",
        "* The number of outputs per input is 200.\n",
        "\n",
        "<br>\n",
        "\n",
        "Two `Dense` layers with a number of neurons and activation function that you choose. We recommend you try a few options.\n",
        "\n",
        "<br>\n",
        "\n",
        "A `Dense` layer for outputting classification probabilities for each of the possible categories.\n",
        "\n",
        "\n",
        "*Hint: If you're not sure which activation function to use, use `relu`.*"
      ],
      "metadata": {
        "id": "_ULrKEoKiKE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim=10000, output_dim=200))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "qliSJAYCiKE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.5: Compile and fit the model**\n",
        "\n",
        "Using standard parameters for classification, compile and train 8yncthis neural network using:\n",
        "* A learning rate of 0.01.\n",
        "* A batch size of 200.\n",
        "* 5 epochs."
      ],
      "metadata": {
        "id": "RMhg0mcliKE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate = 0.01)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=200, epochs=5)"
      ],
      "metadata": {
        "id": "GIWQr3UFiKE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ae59ce-42ca-4733-8b83-e43d485bac89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "9/9 [==============================] - 5s 187ms/step - loss: 1.1841 - accuracy: 0.5292\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 2s 185ms/step - loss: 0.0324 - accuracy: 0.9938\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 2s 165ms/step - loss: 0.0065 - accuracy: 0.9983\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 2s 170ms/step - loss: 0.0279 - accuracy: 0.9961\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 2s 191ms/step - loss: 0.0335 - accuracy: 0.9978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d183c2598d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1.6: Evaluate the model**\n",
        "\n",
        "\n",
        "Now, evaluate the model for both the training and test sets."
      ],
      "metadata": {
        "id": "rZDdIc2LiKE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Loss:\", test_loss)"
      ],
      "metadata": {
        "id": "rtdeiWm7iKE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364d62cc-eead-41c6-a408-0d45d54e30a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0\n",
            "Training Loss: 1.4173429008224048e-05\n",
            "Test Accuracy: 0.9123595356941223\n",
            "Test Loss: 1.3674368858337402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"p2\"></a>\n",
        "\n",
        "---\n",
        "## **Part 2:  Convolutional Neural Networks**\n",
        "---\n",
        "\n",
        "In this section, you will classify the articles using neural networks with `Conv1D` and `MaxPooling1D` hidden layers. Feel free to include `Dense` hidden layers too."
      ],
      "metadata": {
        "id": "R-rxKJXJFu-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #2.1: Create the highest performing model possible using CNNs**\n",
        "\n",
        "\n",
        "Complete the code below to train a new model that is identical to the one above, except using any or all of the CNN layers that keras provides. The goal is to create a model that performs as well as possible on the *test set*."
      ],
      "metadata": {
        "id": "Ek9OyYc_8z6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim=2000, output_dim=128))\n",
        "model.add(Conv1D(filters=63, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Printing Structure\n",
        "for layer in model.layers:\n",
        "  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "# Fitting\n",
        "opt = Adam(learning_rate = 0.01)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=200, epochs=5)\n",
        "\n",
        "# Evaluating\n",
        "print(\"\\n\\n\\n\")\n",
        "model.evaluate(x_train, y_train, verbose=0)\n",
        "model.evaluate(x_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "IqhKG-xN8z6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4d0e5a-9a87-4bd7-c097-101a4e3c0c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1) -> (None, 20)\n",
            "(None, 20) -> (None, 20, 128)\n",
            "(None, 20, 128) -> (None, 18, 63)\n",
            "(None, 18, 63) -> (None, 9, 63)\n",
            "(None, 9, 63) -> (None, 567)\n",
            "(None, 567) -> (None, 64)\n",
            "(None, 64) -> (None, 5)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 3s 186ms/step - loss: 1.3978 - accuracy: 0.3815\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.2268 - accuracy: 0.9388\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 0.0366 - accuracy: 0.9904\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 102ms/step - loss: 0.0186 - accuracy: 0.9972\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 135ms/step - loss: 0.0290 - accuracy: 0.9949\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.267504334449768, 0.8853932619094849]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"p3\"></a>\n",
        "\n",
        "---\n",
        "## **Part 3: [OPTIONAL] IMDB Sentiment Classification**\n",
        "---\n",
        "\n",
        "In this part we will focus on building a CNN model using the IMDB sentiment classification dataset. This is a dataset of 25,000 movie reviews with sentiment labels: 0 for negative and 1 for positive.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Run the code provided below to import the dataset.**"
      ],
      "metadata": {
        "id": "Tnnv9dfa0I-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTdgncgNHtppfS89LHOh1kGl5tYzoEUrUwmOPOQF7mQ0U5Rzba27H45imvZ06_J2x0-wCJySylP5V3_/pub?gid=1712575053&single=true&output=csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size = 0.2, random_state = 42)\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "ACX4sfJW0bq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3.1: Create the `TextVectorization` layer**\n",
        "\n",
        "\n",
        "To get started, let's create a `TextVectorization` layer to vectorize this data.\n",
        "\n",
        "Specifically,\n",
        "1. Initialize the layer with the specified parameters.\n",
        "\n",
        "2. Adapt the layer to the training data.\n",
        "\n",
        "3. Look at the newly built vocabulary."
      ],
      "metadata": {
        "id": "HuGSwhDc0e1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Initialize the layer with the specified parameters.**\n",
        "\n",
        "* The vocabulary should be at most 5000 words.\n",
        "* The layer's output should always be 64 integers."
      ],
      "metadata": {
        "id": "JlbOhj9L0hdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = 5000,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 64\n",
        "  )"
      ],
      "metadata": {
        "id": "r4CA7F9W0l4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Adapt the layer to the training data.**"
      ],
      "metadata": {
        "id": "3SgPJHmy0plM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(x_train)"
      ],
      "metadata": {
        "id": "sGMMXLG50ruI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Look at the newly built vocabulary.**"
      ],
      "metadata": {
        "id": "7KWwtA--0ue-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "t_7qWqMl0xAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd805eae-633b-4dc7-cbc4-642cedd283a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'it',\n",
              " 'i',\n",
              " 'this',\n",
              " 'that',\n",
              " 'br',\n",
              " 'was',\n",
              " 'as',\n",
              " 'for',\n",
              " 'with',\n",
              " 'movie',\n",
              " 'but',\n",
              " 'film',\n",
              " 'on',\n",
              " 'not',\n",
              " 'you',\n",
              " 'are',\n",
              " 'his',\n",
              " 'have',\n",
              " 'be',\n",
              " 'he',\n",
              " 'one',\n",
              " 'its',\n",
              " 'at',\n",
              " 'all',\n",
              " 'by',\n",
              " 'an',\n",
              " 'they',\n",
              " 'from',\n",
              " 'who',\n",
              " 'so',\n",
              " 'like',\n",
              " 'just',\n",
              " 'or',\n",
              " 'her',\n",
              " 'about',\n",
              " 'if',\n",
              " 'has',\n",
              " 'out',\n",
              " 'some',\n",
              " 'there',\n",
              " 'what']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3.2: Build and Train a Dense model**\n",
        "\n",
        "Complete the code below to build a model with the following layers.\n",
        "\n",
        "An Embedding layer such that:\n",
        "- The vocabulary contains 5000 tokens.\n",
        "- The input length corresponds to the output of the vectorization layer.\n",
        "- The number of outputs per input is 128.\n",
        "\n",
        "<br>\n",
        "\n",
        "Hidden layers such that:\n",
        "\n",
        "- There's at least one Dense layer.\n",
        "\n",
        "<br>\n",
        "\n",
        "A Dense layer for outputting classification probabilities for \"negative\" or \"positive\" labels."
      ],
      "metadata": {
        "id": "XH9dVdBW1D4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim=5000, output_dim=128))\n",
        "model.add(Conv1D(filters=63, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Printing Structure\n",
        "for layer in model.layers:\n",
        "  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "# Fitting\n",
        "opt = Adam(learning_rate = 0.01)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=200, epochs=5)\n",
        "\n",
        "# Evaluating\n",
        "print(\"\\n\\n\\n\")\n",
        "model.evaluate(x_train, y_train, verbose=0)\n",
        "model.evaluate(x_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "tr-vmrCI1_mU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f5637f-185e-44b3-a788-b6db7dc46a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1) -> (None, 64)\n",
            "(None, 64) -> (None, 64, 128)\n",
            "(None, 64, 128) -> (None, 62, 63)\n",
            "(None, 62, 63) -> (None, 31, 63)\n",
            "(None, 31, 63) -> (None, 1953)\n",
            "(None, 1953) -> (None, 64)\n",
            "(None, 64) -> (None, 2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 17s 75ms/step - loss: 0.5824 - accuracy: 0.6863\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.4333 - accuracy: 0.7992\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.3339 - accuracy: 0.8586\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 4s 22ms/step - loss: 0.2182 - accuracy: 0.9158\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1271 - accuracy: 0.9581\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7545827627182007, 0.7677000164985657]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This other alternative includes building the model with CNN.\n",
        "**Which architecture performs better?**"
      ],
      "metadata": {
        "id": "_UHdf2m15v1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [OPTIONAL] USING CNNs\n",
        "model = Sequential()\n",
        "\n",
        "# Input, Vectorization, and Embedding Layers\n",
        "model.add(Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Conv1D(filters = 16, kernel_size = 4, activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = 3))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(2, activation = 'softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Printing Structure\n",
        "for layer in model.layers:\n",
        "  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Fitting\n",
        "opt = Adam(learning_rate = 0.001)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n",
        "\n",
        "\n",
        "# Evaluating\n",
        "print(\"\\n\\n\\n\")\n",
        "model.evaluate(x_train, y_train)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSM9qX412zyL",
        "outputId": "eb26946f-1562-492d-c313-8cad38733c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1) -> (None, 64)\n",
            "(None, 64) -> (None, 64, 128)\n",
            "(None, 64, 128) -> (None, 61, 16)\n",
            "(None, 61, 16) -> (None, 20, 16)\n",
            "(None, 20, 16) -> (None, 320)\n",
            "(None, 320) -> (None, 2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 15s 85ms/step - loss: 0.5821 - accuracy: 0.6798\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.3865 - accuracy: 0.8257\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 4s 28ms/step - loss: 0.3193 - accuracy: 0.8640\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.2461 - accuracy: 0.9081\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 4s 24ms/step - loss: 0.1599 - accuracy: 0.9539\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0864 - accuracy: 0.9908\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.5091 - accuracy: 0.7856\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5091047286987305, 0.7856000065803528]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###© 2024 The Coding School, All rights reserved"
      ],
      "metadata": {
        "id": "8zFgD9Jx1ooV"
      }
    }
  ]
}